# frontend/app.py
import os, sys, pickle, joblib
import streamlit as st
import pandas as pd

# â€”â€”â€” Ensure we can import backend modules (assumes ../backend exists) â€”â€”â€”
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

try:
    import cloudpickle
except ImportError:
    cloudpickle = None  # optional

# â€”â€”â€” Backend audit modules â€”â€”â€”
from backend.bias_audit import audit_fairness
from backend.drift_detection import detect_data_drift
from backend.robustness import evaluate_robustness
from backend.explainability import explain_with_shap_ver2

# ----------------- Streamlit Page Config -----------------
st.set_page_config(page_title="SmartÂ AIÂ Auditor", layout="wide")
st.title("ğŸ§ â€¯SmartÂ AIÂ Auditor")

# ----------------- Sidebar: Uploads -----------------
st.sidebar.header("Upload Model & Data")
model_file = st.sidebar.file_uploader("Model (.pkl)", type=["pkl"])
data_file  = st.sidebar.file_uploader("Dataset (.csv)", type=["csv"])
label_col  = st.sidebar.text_input("Label Column (ground truth)", value="label")

# ----------------- Model Loader -----------------
def load_model(f):
    loaders = [
        ("joblib",       joblib.load),
        ("pickle",       pickle.load),
        ("cloudpickle",  cloudpickle.load if cloudpickle else None)
    ]
    for name, loader in loaders:
        if loader is None:
            continue
        try:
            f.seek(0)
            return loader(f)
        except Exception as e:
            st.warning(f"â€¢Â { name } loader failed: { e }")
    return None

# ----------------- Main Logic -----------------
if model_file and data_file:
    model = load_model(model_file)
    if model is None:
        st.error("âŒÂ Could not load model. Make sure it was saved with joblib / pickle / cloudpickle.")
        st.stop()

    df = pd.read_csv(data_file)
    if label_col not in df.columns:
        st.error(f"âŒÂ Label column '{label_col}' not in dataset.")
        st.stop()

    X, y = df.drop(columns=[label_col]), df[label_col]
    st.success("âœ…Â Model & data loaded!")

    # ----------------- Select Audit -----------------
    audit_choice = st.selectbox("Choose an audit to run", 
                                ["Bias / Fairness", "ExplainabilityÂ (SHAP)", "DataÂ Drift", "Robustness"])

    # -------------- Bias / Fairness -----------------
    if audit_choice == "Bias / Fairness":
        st.subheader("âš–ï¸Â Bias & Fairness Audit")
        sensitive_cols = st.sidebar.multiselect("Sensitive attribute(s)", options=X.columns.tolist())
        if not sensitive_cols:
            st.warning("Select at least one sensitive column in the sidebar.")
        else:
            y_pred = model.predict(X)
            for col in sensitive_cols:
                st.markdown(f"### ğŸ”Â Results for `{col}`")
                result = audit_fairness(y_true=y, y_pred=y_pred, sensitive_features=X[col])

                st.write("**Accuracy by Group**", result["accuracy_by_group"])
                st.write("**SelectionÂ Rate by Group**", result["selection_rate_by_group"])
                st.markdown(f"**Verdict:** { result['fairness_interpretation']['notes'] }")
                with st.expander("Details"):
                    st.json(result["fairness_interpretation"])
                st.divider()

    # -------------- Explainability (SHAP) -----------
    elif audit_choice == "ExplainabilityÂ (SHAP)":
        st.subheader("ğŸ§ Â SHAP Explainability")
        sample = X.sample(n=min(5, len(X)), random_state=42)
        explanation = explain_with_shap_ver2(model, sample)
        if "figures" in explanation:
            for fig in explanation["figures"]:
                st.pyplot(fig)
        else:
            st.error(explanation.get("error", "SHAP explanation failed."))

    # -------------- DataÂ Drift -----------------------
    elif audit_choice == "DataÂ Drift":
        st.subheader("ğŸ“‰Â DataÂ Drift Detection")
        ref = X.sample(frac=0.5, random_state=1)
        cur = X.sample(frac=0.5, random_state=2)
        report = detect_data_drift(ref, cur)
        st.metric("Drift Score", f"{report['drift_score']:.2f}")
        st.write(report["notes"])
        with st.expander("Featureâ€‘level stats"):
            st.json(report["feature_stats"])

    # -------------- Robustness -----------------------
    elif audit_choice == "Robustness":
        st.subheader("ğŸ›¡ï¸Â Robustness Test")
        res = evaluate_robustness(model, X, y)
        if "error" in res:
            st.error(res["error"])
        else:
            st.metric("OriginalÂ Accuracy", f"{res['original_accuracy']:.3f}")
            st.metric("NoisyÂ Accuracy",    f"{res['noisy_accuracy']:.3f}")
            st.metric("AccuracyÂ Drop",     f"{res['accuracy_drop']:.3f}")
            st.write(res["notes"])
else:
    st.info("ğŸ‘ˆÂ Upload a model and dataset to get started.")
